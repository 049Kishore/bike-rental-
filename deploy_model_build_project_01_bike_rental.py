# -*- coding: utf-8 -*-
"""Deploy_Model_Build_Project_01-Bike_Rental.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/173x6fYrQUIBfxRT6L8H5m-ugr1sXWUy2

#PRCP-1018-BikeRental
"""

from google.colab import drive
drive.mount('/content/drive')

"""# **Problem Statement**




Task 1:- Prepare a complete data analysis report on the given data.






Task 2:- Prediction of daily bike rental count based on the environmental and
seasonal settings.

# **Attribute Information:**

Both the files hour.csv and day.csv have the following fields, except hr which is not
available in day.csv
- instant: record index
- dteday : date
- season : season (1:winter, 2:spring, 3:summer, 4:fall)

- yr : year (0: 2011, 1:2012)
- mnth : month ( 1 to 12)
- hr : hour (0 to 23)
- holiday : weather day is holiday or not (extracted from [Web Link])
- weekday : day of the week
- workingday : if neither weekend nor holiday, it is 1; otherwise 0.
+ weathersit :
- 1: Clear, Few clouds, Partly cloudy, Partly cloudy
- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered
clouds
- 4: Heavy Rain + Ice Pellets + Thunderstorm + Mist, Snow + Fog
- temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-
t_min), t_min=-8, t_max=+39 (only in hourly scale)
- atemp: Normalized feeling temperature in Celsius. The values are derived via (t-
t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)
- hum: Normalized humidity. The values are divided to 100 (max)
- windspeed: Normalized wind speed. The values are divided to 67 (max)
- casual: count of casual users
- registered: count of registered users
- cnt: count of total rental bikes including both casual and registered

# Data Preprocessing

1.   List item
2.   List item

## Library Import
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""## Data Import"""

data_1 = pd.read_csv("/content/day.csv")
data_2 = pd.read_csv("/content/hour.csv")

"""from google.colab import drive
drive.mount('/content/drive')"""

data_1.head()

data_1.shape

data_1.info()

data_1.describe()

# data_2

data_2.head()

data_2.shape

data_2.info()

data_2.describe()

"""# Feature Engineering"""

# Join Two Datasets using concat

concat_df = pd.concat([data_2,data_1],axis=0,ignore_index=True)
print(f'Shape of dataset after concat:{concat_df.shape}')
concat_df.tail()

concat_df.isnull().sum()

na_index = concat_df[concat_df['hr'].isna()].index
print(na_index)

for i, n in enumerate(na_index):
    concat_df.at[n,'hr'] = i % 24

concat_df.isnull().sum()

concat_df.tail()

concat_df['hr'] = concat_df['hr'].astype(int)

concat_df['dteday'] = pd.to_datetime(concat_df['dteday'])

concat_df.info()

concat_df.describe()

"""## Detecting Outliers"""

plt.figure(figsize=(10,6))
plt.subplot(3,1,1)
sns.boxplot(data=concat_df,x='casual')
plt.title('Outliers in Casuals')

plt.subplot(3,1,2)
sns.boxplot(data=concat_df,x='registered')
plt.title('Outliers in Registered')

plt.subplot(3,1,3)
sns.boxplot(data=concat_df,x='cnt')
plt.title('Outliers in Counts')

plt.tight_layout()
plt.show()

time_df = concat_df.copy()

time_df.shape

Q1 = time_df['cnt'].quantile(0.25)
Q3 = time_df['cnt'].quantile(0.75)
print("Q1:",Q1)
print("Q3:",Q3)

IQR = Q3-Q1

low_bound = Q1 - 1.5 * IQR
up_bound = Q3 + 1.5 * IQR
print("Lower limit:",low_bound)
print("Upper limit:",up_bound)

ts_df = time_df[(time_df['cnt']>=low_bound)&(time_df['cnt']<=up_bound)]

ts_df.describe()

print(f'Total Datas before eliminating Outliers:{time_df.shape}')
print(f'Total Datas after eliminating Outliers:{ts_df.shape}')

"""## Time Series Analysis"""

from statsmodels.tsa.stattools import adfuller

result = adfuller(ts_df['cnt'])

print(f'Test Statistics: {result[0]}')
print(f'p-value: {result[1]}')
print(f'Critial Values: {result[4]}')

if result[1] > 0.05 :
  print('Series is not Stationary')
else:
  print('Series is Stationary')

from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

df_time = ts_df[['dteday', 'cnt']]

df_time.set_index('dteday',inplace=True)

plot_acf(df_time, lags=30)
plot_pacf(df_time, lags=30)
plt.show()

"""Insights from the PACF Plot:

    Significant Spike at Lag 1:

        Thereâ€™s a strong spike at lag 1, and a rapid drop afterwards.

        This suggests a first-order autoregressive (AR(1)) component.
"""

from statsmodels.tsa.arima.model import ARIMA

model = ARIMA(df_time, order=(1, 0, 2))  # ARIMA(p=1, d=0, q=2)
model_fit = model.fit()
print(model_fit.summary())

df_time.iloc[17050:]

forecast = model_fit.forecast(steps=10)  # Forecast next 10 steps
print(forecast)
forecast.plot()
plt.show()

"""## Feature Selection"""

sns.heatmap(ts_df.corr(),annot=True,fmt='.1f',cmap='coolwarm')

"""## Statistics"""

# variance_inflation_factor
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# Load your DataFrame (assuming it's named df)
# Drop columns that are not numerical or are IDs/dates
X = ts_df.drop(columns=['instant','dteday','cnt'])

# Add constant for intercept
X = add_constant(X)

# Calculate VIF
vif_data = pd.DataFrame()
vif_data['feature'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print(vif_data)

"""From the above VIF result 'temp' and 'atemp' have very high mulicollinearity and so we will drop those columns"""

ts_df.columns

# For ML Algoritms
final_df = ts_df.drop(columns=['instant','dteday','temp','atemp'],axis=1)

final_df.shape

final_df.columns

sns.boxplot(data=final_df,x='cnt')

final_df['cnt'].skew()

sns.histplot(data=final_df['cnt'],kde=True);

"""Left Skew

# Data Visualization
"""

sns.barplot(data=final_df,x='season',y='cnt')
plt.title('Bike Rental Count based on Seasons')
plt.show()

"""1 -- > Winter

2 -- > Spring

3 -- > Summer

4 -- > Fall

Bike Rental Count attains maximum in Summer Season. Though rental count is low in Winter.
"""

sns.barplot(data=final_df,x='weathersit',y='cnt')
plt.title('Bike Rental Count based on Weather conditions')
plt.show()

"""1 -- > Clear Weather

2 -- > Mist+Cloudy

3 -- > Light snow

4 -- > Heavy Rain

Bike Rental Count attains maximum in Clear weather conditions. Though rental count is low when it is heavy rain.
"""

sns.barplot(data=final_df,x='weekday',y='cnt')
plt.title('Bike Rental Count based on Days')
plt.show()

"""Bike Retal Counts are high in Weekend days

# Model Building
"""

X = final_df.drop(columns = 'cnt',axis=1)
y = final_df['cnt']
print('X:',X.shape)
print('Y:',y.shape)

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)

print(f'Training Data: {X_train.shape}')
print(f'Testing Data: {X_test.shape}')

"""from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV """

# Random Forest
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error,root_mean_squared_error

"""## Model Prediction"""

rfr = RandomForestRegressor(criterion='poisson', min_samples_leaf=1, min_samples_split=2)
rfr.fit(X_train,y_train)
y_pred = rfr.predict(X_test)
print(r2_score(y_test,y_pred))
print(mean_squared_error(y_test,y_pred))
print(mean_absolute_error(y_test,y_pred))
print(root_mean_squared_error(y_test,y_pred))

"""Model Prediction using test values"""

print(X_test.iloc[50:55])

print(y_test.iloc[50:55])

import warnings
warnings.filterwarnings('ignore')

# Random Forest
rfr = RandomForestRegressor(criterion='poisson', min_samples_leaf=1, min_samples_split=2)
rfr.fit(X_train,y_train)
new_pred_4 = rfr.predict([[3,1,6,2,0,5,1,1,0.79,0.00,0,12]])

print("Random Forest Prediction Result:",int(new_pred_4))

import joblib
joblib.dump(rfr,'Bike_Rental_count.pkl')

['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday',
       'weathersit', 'hum', 'windspeed', 'casual', 'registered', 'cnt']

# prompt: with above model get some input features and predict the cnt

import ipywidgets as widgets
from IPython.display import display, clear_output
#import pandas as pd
#import numpy as np
from sklearn.ensemble import RandomForestRegressor
import plotly.graph_objs as go
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Assuming `merged_df` is already loaded and processed as in the preceding code
model = RandomForestRegressor()
# Step 1: Define Input Widgets
season_widget = widgets.Dropdown(options=sorted(final_df['season'].unique()), description='Season:')
yr_widget = widgets.Dropdown(options=sorted(final_df['yr'].unique()), description='Year:')
mnth_widget = widgets.Dropdown(options=sorted(final_df['mnth'].unique()), description='Month:')
hr_widget = widgets.Dropdown(options=sorted(final_df['hr'].unique()), description='Hour:')
holiday_widget = widgets.Dropdown(options=sorted(final_df['holiday'].unique()), description='Holiday:')
weekday_widget = widgets.Dropdown(options=sorted(final_df['weekday'].unique()), description='Weekday:')
workingday_widget = widgets.Dropdown(options=sorted(final_df['workingday'].unique()), description='Workingday:')
weathersit_widget = widgets.Dropdown(options=sorted(final_df['weathersit'].unique()), description='Weather Situation:')
hum_widget = widgets.FloatSlider(min=final_df['hum'].min(), max=final_df['hum'].max(), step=0.01, description='Humidity:')
windspeed_widget = widgets.FloatSlider(min=final_df['windspeed'].min(), max=final_df['windspeed'].max(), step=0.01, description='Windspeed:')
casual_widget = widgets.IntSlider(min=0, max=int(final_df['casual'].max()), step=1, description='Casual:')
registered_widget = widgets.IntSlider(min=0, max=int(final_df['registered'].max()), step=1, description='Registered:')
predict_button = widgets.Button(description="Predict Count")
output_widget = widgets.Output()


# Step 2: Define Prediction Function
def predict_cnt(b):
  with output_widget:
    clear_output()
    try:
      # Create input dataframe from widget values
      input_data = pd.DataFrame({
          'season': [season_widget.value],
          'yr': [yr_widget.value],
          'mnth': [mnth_widget.value],
          'hr': [hr_widget.value],
          'holiday': [holiday_widget.value],
          'weekday': [weekday_widget.value],
          'workingday': [workingday_widget.value],
          'weathersit': [weathersit_widget.value],
          'hum': [hum_widget.value],
          'windspeed': [windspeed_widget.value],
          'casual': [casual_widget.value],
          'registered': [registered_widget.value]
      })

      # Ensure the columns are in the same order as the training data
      # This is crucial for consistent predictions
      training_features = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday',
                            'weathersit', 'hum', 'windspeed', 'casual', 'registered']
      input_data = input_data[training_features]


      # Predict using the trained XGBoost model
      predicted_count = rfr.predict(input_data)

      print(f"Predicted Daily Bike Rental Count: {int(predicted_count)}")

    except Exception as e:
      print(f"An error occurred: {e}")

# Step 3: Link Button to Function
predict_button.on_click(predict_cnt)

# Step 4: Display Widgets
display(season_widget, yr_widget, mnth_widget,hr_widget, holiday_widget, weekday_widget,
        workingday_widget, weathersit_widget,hum_widget, windspeed_widget,
        casual_widget, registered_widget, predict_button, output_widget)

"""## Report on Challenges Faced

1. Concat Two Tables and replacing null records.
2. Detecting Outliers and eliminating it.
3. Decision on Features Selection.
4. Hyperparameter Tunning technique consumes more time especially for Random Forest Algorithm, though the size of records are high.

Batch : 16-DEC-24-CDS-BUN-021-WDM0900-CBE

 Project Team ID : PTID-CDS-MAY-25-2668

 Project Team Members :

*   Arunachalam
*   Kishore
*   Tharun
*   Sriram
"""